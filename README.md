# Machine_Learning_Algorithms

## Linear Regression - 
#### https://github.com/JayaVerma/Machine_Learning_Algorithms/blob/main/Linear%20Regression%20Practical%20Example%20.ipynb
#### Descrption: 
- Supervised learning algorithm that fits a linear equation based on the training data. 
- The equation is used for predictions on new data.
#### Used For:
- Regression
#### inputs:
- Numerical and categorical both
#### Handles:
- Small dataset
- Medium dataset
- Large dataset
#### Processing:
- Standardizing the inputs
- Removing irrelevant features
#### Algorithm speed:
- Traning: Fast
- Testing: Fast
#### Avoid Overfitting: 
- Regularization
#### Pros:
- Intuitive
- Easy to interpret.
- Works very well with linear data
#### cons:
- Assumes linearity between dependent and independent variables, otherwise it performs weakly
- Sensitive to outliers
#### Applications:
- Forcasting
- Medical Research
- Trend evaluation
- Variable dependencies
- Variable dependencies estimate


## Logistic Regression - 
#### https://github.com/JayaVerma/Machine_Learning_Algorithms/blob/main/logistic%20regression%20Notebook.ipynb
#### Descrption: 
Classification algorithm that predicts the probability of an event occurring using a logistic function. 
Logistic regression can transform into its logit form, where the log of the odds is equal to a linear model. 
Applied whenever we want to predict categorical outputs
#### Used For:
- Classification
#### inputs:
Numerical and categorical both
#### Handles:
Small dataset
Medium dataset
Large dataset
#### Processing:
- Standardizing the inputs
- Removing irrelevant features
#### Algorithm speed:
- Traning: Fast
- Testing: Fast
#### Avoid Overfitting: 
- Regularization
#### Pros:
- Intuitive
- Easy to interpret,
- Easy to implemenP
- Does not require a linear relationship between the dependent and independent variable
- Shows which predictors are important
#### cons: 
- Prone to overfitting when dealing with high-dimensional data (the curse of dimensionality)
- Requires large sample sizes
- Limited to linearly separable problems
#### Applications:
- Medical Research
- Gaming
- Text Editing
- -Advertising
- Credit card default pridiction

## Ridge and Lasso Regression - 
#### https://github.com/JayaVerma/Machine_Learning_Algorithms/blob/main/Multiple%20Linear_Regression_Ridge_Lasso_Hitters.ipynb
#### Descrption: 
- Regression algorithm that applies regularization to deal with overfitted data.
The method uses L2 regularization.

- Regression algorithm that applies regularization and feature selection to deal with overfitted data.
The method uses L1 regularization.
#### Used For: 
Regression
#### inputs:
Numerical and categorical both
#### Handles:
Small dataset
Medium dataset
Large dataset
#### Processing:
- Standardizing the inputs
- Removing irrelevant features
#### Algorithm speed:
- Traning: Fast
- Testing: Fast
#### Avoid Overfitting:
- Adjust the penalty Term
#### Pros(RIDGE):
- Prevents overfitting and multicollinearity issue.
- Lowers variance
#### Pros(LASSO):
- Prevents overfitting and multicollinearity issues
- Lowers variance 
- Performs feature selection
#### cons(RIDGE):
- Increase bias
- Sensitive to irrelevent features
- Difficult to interpret the corff in the final model
#### Applications:
- Generic study
- water resource supply
- Clinical measures
- House Price Prediction
- Forcasting
- Time series prediction
- Applied test testing



## K-Mean Clustering - 
#### https://github.com/JayaVerma/Machine_Learning_Algorithms/blob/main/Market%20Segmentation%20with%20Cluster%20Analysis%20.ipynb
#### Descrption: 
#### Used For:
#### inputs:
Numerical and categorical both
#### Handles:
Small dataset
Medium dataset
Large dataset
#### Processing:
#### Algorithm speed:
#### Avoid Overfitting:
#### Pros:
#### cons:
#### Applications:

